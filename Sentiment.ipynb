{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import keras\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "import os\n",
    "swords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and pre - processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"I was so moved by this film in 1981, I went back to the theater four times to see it again! Something I have never done for another film. No movie evokes the feelings of growing up in the 60's like Four Friends. That it so closely approximated my own experiences in the 60's is probably something that many will share. Jodi Thelen is radiantly beautiful and unforgetable! Why she didn't become a major star after this I will never know. The acting by the entire cast is flawless as is Steve Tisch's script. I always wanted to know how much of the story was autobiographical. But alas, Steve is no longer here to answer that question. I have all but worn out my VHS copy of this great movie! Highly recommended!\", 1)\n",
      "\n",
      "('moved film 1981 went back theater four times see something never done another film movie evokes feelings growing 60s like four friends closely approximated experiences 60s probably something many share jodi thelen radiantly beautiful unforgetable didnt become major star never know acting entire cast flawless steve tischs script always wanted know much story autobiographical alas steve longer answer question worn vhs copy great movie highly recommended', 1)\n"
     ]
    }
   ],
   "source": [
    "neg_list = os.listdir(\"neg_test/\")\n",
    "pos_list = os.listdir(\"pos_test/\")\n",
    "\n",
    "tot_text = []\n",
    "for i in pos_list:\n",
    "    fp = open(\"pos_test/\"+i,\"r\")\n",
    "    tot_text.append((fp.read(),1))\n",
    "for i in neg_list:\n",
    "    fp = open(\"neg_test/\"+i,\"r\")\n",
    "    tot_text.append((fp.read(),0))\n",
    "\n",
    "print(tot_text[0])\n",
    "\n",
    "clean_text = [([\"\".join([k.lower() for k in list(j) if k not in punctuation]) for j in i[0].split()],i[1]) for i in tot_text]\n",
    "clean_text = [(\" \".join(list(filter(lambda x:x not in swords,i[0]))),i[1]) for i in clean_text]\n",
    "print()\n",
    "print(clean_text[0])\n",
    "\n",
    "text0map = [i[0] for i in clean_text]\n",
    "text1map = [i[1] for i in clean_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_list = os.listdir(\"neg_train/\")\n",
    "pos_list = os.listdir(\"pos_train/\")\n",
    "\n",
    "tot_text = []\n",
    "for i in pos_list:\n",
    "    fp = open(\"pos_train/\"+i,\"r\")\n",
    "    tot_text.append((fp.read(),1))\n",
    "for i in neg_list:\n",
    "    fp = open(\"neg_train/\"+i,\"r\")\n",
    "    tot_text.append((fp.read(),0))\n",
    "\n",
    "\n",
    "clean_text = [([\"\".join([k.lower() for k in list(j) if k not in punctuation]) for j in i[0].split()],i[1]) for i in tot_text]\n",
    "clean_text = [(\" \".join(list(filter(lambda x:x not in swords,i[0]))),i[1]) for i in clean_text]\n",
    "\n",
    "\n",
    "text0map0 = [i[0] for i in clean_text]\n",
    "text1map1 = [i[1] for i in clean_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer to convert words to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1470, 3, 5214, 295, 55, 651, 570, 117, 11, 52, 35, 126, 63, 3, 2, 7023, 1240, 1698, 1583, 5, 570, 229, 3011, 61511, 2257, 1583, 136, 52, 31, 1359, 19827, 45461, 32474, 201, 51714, 60, 304, 519, 235, 35, 41, 37, 314, 86, 3566, 1191, 81606, 131, 106, 341, 41, 13, 12, 12880, 2953, 1191, 1009, 1321, 779, 4926, 1650, 825, 16, 2, 417, 1015]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text0map + text0map0) \n",
    "sequence = tokenizer.texts_to_sequences(text0map + text0map0)\n",
    "print(sequence[0])\n",
    "get_word_index(\"something\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mayank/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/mayank/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 190s 8ms/step - loss: 0.4392 - acc: 0.7947 - val_loss: 0.3956 - val_acc: 0.8295\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 224s 9ms/step - loss: 0.3436 - acc: 0.8529 - val_loss: 0.3511 - val_acc: 0.8572\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 218s 9ms/step - loss: 0.2683 - acc: 0.8945 - val_loss: 0.3228 - val_acc: 0.8641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe06704fe48>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.408\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy:\",(scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text back to integers for translating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = keras.datasets.imdb.get_word_index()\n",
    "\n",
    "id_to_word = {v:k for k,v in word_to_id.items()}    \n",
    "id_to_word[0] = \"<pad>\"\n",
    "id_to_word[1] = \"<empty>\"\n",
    "id_to_word[2] = \"<na>\"\n",
    "  \n",
    "full_text = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25000):\n",
    "    restored_text = \" \".join([id_to_word[j] for j in X_test[i]])\n",
    "    full_text[restored_text] = X_test[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictText(text):\n",
    "    \n",
    "    \n",
    "    convert = [word_to_id[i] for i in text.split() if i in word_to_id]\n",
    "    if len(convert)<500:\n",
    "        convert += [0 for i in range(500-len(convert))]\n",
    "    else:\n",
    "        convert = convert[:500]\n",
    "    \n",
    "    sentiment = clf.predict([np.array(convert)])\n",
    "    \n",
    "    if sentiment:\n",
    "        return \"Positive\"\n",
    "    return \"Negative\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictText(\"this is so bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  50.032\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "wrong = 0\n",
    "val = 0\n",
    "for i in range(len(text0map)):\n",
    "\n",
    "    txt = predictText(text0map[i])\n",
    "    if txt==\"Positive\":\n",
    "        val=1\n",
    "    else:\n",
    "        val=0\n",
    "    if val==text1map[i]:\n",
    "        correct+=1\n",
    "    else:\n",
    "        wrong+=1\n",
    "        \n",
    "print(\"Accuracy : \", correct/(correct+wrong)*100)\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
